{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizing...\n",
      "Thevar speech at Stanford University in 2005 to encourage the gratitude students to follow their heart and do what tailor despite all the difficulties because life is short firstly he puts his main message by telling about the time you dropped out of college only kept courses you are truly interested about Academy it was a student at that he knew what he wanted to do was and he was young and that you should it no matter what Early this year is near to that experience with cancer to show that you have enjoyed every moment in life you never know when you are time will come finally it really did a great job of expression that your work is going to Fir lot part of her life and the only way to be truly satisfied is to do what you believe is great work and the only way to do great work is to love what you do\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    # read the audio data from the default microphone\n",
    "    audio_data = r.record(source, duration=80)\n",
    "    print(\"Recognizing...\")\n",
    "    # convert speech to text\n",
    "    text = r.recognize_google(audio_data)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thevar speech at Stanford University in 2005 to encourage the gratitude students to follow their heart and do what tailor despite all the difficulties because life is short firstly he puts his main message by telling about the time you dropped out of college only kept courses you are truly interested about Academy it was a student at that he knew what he wanted to do was and he was young and that you should it no matter what Early this year is near to that experience with cancer to show that you have enjoyed every moment in life you never know when you are time will come finally it really did a great job of expression that your work is going to Fir lot part of her life and the only way to be truly satisfied is to do what you believe is great work and the only way to do great work is to love what you do'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "    # return the text for all chunks detected\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (r\"C:\\Users\\ssi\\OneDriveTU\\Desktop\\harvard.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-chunks\\chunk1.wav : The still smell of old beer drinkers. \n",
      "audio-chunks\\chunk2.wav : It takes heat to bring out the order. \n",
      "audio-chunks\\chunk3.wav : Call dip research help invest. \n",
      "audio-chunks\\chunk4.wav : I saw pic of this fine. \n",
      "audio-chunks\\chunk5.wav : Thakur guitar store are my favourite. \n",
      "audio-chunks\\chunk6.wav : Just for food is bihar cross ban. \n"
     ]
    }
   ],
   "source": [
    "sampletext =get_large_audio_transcription(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-chunks\\chunk1.wav : The still smell of old beer drinkers. \n",
      "audio-chunks\\chunk2.wav : It takes heat to bring out the order. \n",
      "audio-chunks\\chunk3.wav : Call dip research help invest. \n",
      "audio-chunks\\chunk4.wav : I saw pic of this fine. \n",
      "audio-chunks\\chunk5.wav : Thakur guitar store are my favourite. \n",
      "audio-chunks\\chunk6.wav : Just for food is bihar cross ban. \n",
      "\n",
      "Full text: The still smell of old beer drinkers. It takes heat to bring out the order. Call dip research help invest. I saw pic of this fine. Thakur guitar store are my favourite. Just for food is bihar cross ban. \n"
     ]
    }
   ],
   "source": [
    "path = (r\"C:\\Users\\ssi\\OneDriveTU\\Desktop\\harvard.wav\")\n",
    "print(\"\\nFull text:\", get_large_audio_transcription(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 1.80k/1.80k [00:00<00:00, 440kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:06<00:00, 147kB/s]  \n",
      "Downloading: 100%|██████████| 456k/456k [00:01<00:00, 310kB/s]  \n",
      "Downloading: 100%|██████████| 1.22G/1.22G [02:17<00:00, 8.89MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 87.0/87.0 [00:00<00:00, 83.4kB/s]\n",
      "c:\\Users\\ssi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ssi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 1.39k/1.39k [00:00<00:00, 346kB/s]\n",
      "Downloading: 100%|██████████| 1.91M/1.91M [00:09<00:00, 199kB/s] \n",
      "Downloading: 100%|██████████| 3.52M/3.52M [00:11<00:00, 309kB/s] \n",
      "Downloading: 100%|██████████| 65.0/65.0 [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 2.28G/2.28G [04:15<00:00, 8.90MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "Downloading: 100%|██████████| 1.72k/1.72k [00:00<00:00, 1.72MB/s]\n",
      "c:\\Users\\ssi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ssi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 899k/899k [00:01<00:00, 476kB/s]  \n",
      "Downloading: 100%|██████████| 456k/456k [00:01<00:00, 382kB/s]  \n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:02<00:00, 524kB/s] \n",
      "Downloading: 100%|██████████| 558M/558M [04:22<00:00, 2.13MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "model2 = AutoModel.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer2(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (BartModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BartForConditionalGeneration', 'BartForCausalLM'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ssi\\OneDriveTU\\Desktop\\RESEARCH.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ssi/OneDriveTU/Desktop/RESEARCH.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summary \u001b[39m=\u001b[39m model2\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokens)\n",
      "File \u001b[1;32mc:\\Users\\ssi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ssi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation_utils.py:1206\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    963\u001b[0m \n\u001b[0;32m    964\u001b[0m \u001b[39mGenerates sequences of token ids for models with a language modeling head. The method supports the following\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[39m['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[39m# 0. Validate the `.generate()` call\u001b[39;00m\n\u001b[1;32m-> 1206\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_class()\n\u001b[0;32m   1207\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m   1209\u001b[0m \u001b[39m# 1. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ssi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation_utils.py:889\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[39mif\u001b[39;00m generate_compatible_classes:\n\u001b[0;32m    888\u001b[0m     exception_message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Please use one of the following classes instead: \u001b[39m\u001b[39m{\u001b[39;00mgenerate_compatible_classes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 889\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[1;31mTypeError\u001b[0m: The current model class (BartModel) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'BartForConditionalGeneration', 'BartForCausalLM'}"
     ]
    }
   ],
   "source": [
    "summary = model2.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s><s> The only way to do great work is to love what you do is to follow your heart and do what they love. Steve Job really did a great job of expression that your work is going to to sell a large part of your life and the only way of being truly satisfied is to do what you believe is great work.</s>'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(summary[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
